{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0985b77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/local/lib/python3.8/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.10.0-cp38-cp38-manylinux1_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext) (4.62.1)\n",
      "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.9.0+cpu)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext) (2.26.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.9.0->torchtext) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (1.26.6)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.10.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# !pip install transformers\n",
    "# !pip install torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ea4f8",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e651b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from torchtext.legacy import datasets\n",
    "from torchtext.legacy import data\n",
    "\n",
    "from lib.utils import clean_str\n",
    "from lib.utils import load_data\n",
    "from lib.utils import prepare_data\n",
    "from lib.utils import model \n",
    "\n",
    "from lib.utils import train\n",
    "from lib.utils import evaluate\n",
    "from lib.utils import epoch_time\n",
    "from lib.utils import binary_accuracy\n",
    "from lib.utils import count_parameters\n",
    "from lib.utils import predict_sentiment\n",
    "from lib.utils import BERTGRUSentimentS\n",
    "from lib.utils import BERTGRUSentimentB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 7\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Fonte: https://gist.github.com/nissan/ccb0553edb6abafd20c3dec34ee8099d\n",
    "\n",
    "class DataFrameDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row.sentiment if not is_test else None\n",
    "            text = row.text\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, train_df, val_df=None, test_df=None, **kwargs):\n",
    "        train_data, val_data, test_data = (None, None, None)\n",
    "\n",
    "        if train_df is not None:\n",
    "            train_data = cls(train_df.copy(), text_field, label_field, **kwargs)\n",
    "        if val_df is not None:\n",
    "            val_data = cls(val_df.copy(), text_field, label_field, **kwargs)\n",
    "        if test_df is not None:\n",
    "            test_data = cls(test_df.copy(), text_field, label_field, True, **kwargs)\n",
    "\n",
    "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27519d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29794\n",
      "<bound method PreTrainedTokenizerFast.convert_tokens_to_ids of PreTrainedTokenizerFast(name_or_path='neuralmind/bert-base-portuguese-cased', vocab_size=29794, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})>\n",
      "[CLS] [SEP] [PAD] [UNK]\n",
      "101 102 0 100\n",
      "Carregando dataset.\n",
      "Dataset carregado.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=True)\n",
    "\n",
    "print(len(tokenizer.vocab))\n",
    "print(tokenizer.convert_tokens_to_ids)\n",
    "\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)\n",
    "\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n",
    "\n",
    "max_input_length = 300\n",
    "\n",
    "parameters = {}\n",
    "parameters['model_filename'] = 'model/model_pt-bi-lstm.h5' # O model será exportado para este arquivo\n",
    "parameters['pre_trained_wv'] = False\n",
    "parameters['bilstm'] = True # LSTM Bidirectional True or False\n",
    "\n",
    "parameters['dataset_file'] = './dataset/data_imdb_en_pt.csv'\n",
    "parameters['lang'] = 'pt' # pt or en\n",
    "parameters['load_from'] = 'ftr' # csv or ftr\n",
    "\n",
    "parameters['epochs'] = 5\n",
    "\n",
    "parameters['word_embedding_dim'] = 50 # dimensionalidade do word embedding pré-treinado\n",
    "parameters['batch_size'] = 32 # número de amostras a serem utilizadas em cada atualização do gradiente\n",
    "parameters['max_features'] = 5000 # Reflete a quantidade máxima de palavras que iremos manter no vocabulário\n",
    "parameters['embed_dim'] = 128 # dimensão de saída da camada Embedding\n",
    "parameters['max_sequence_length'] = 300 # limitamos o tamanho máximo de todas as sentenças\n",
    "\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens\n",
    "\n",
    "df = load_data(parameters)\n",
    "df.sentiment.replace({\"neg\": 0, \"pos\": 1}, inplace = True)\n",
    "\n",
    "X, X_test, Y, Y_test = train_test_split(df.text,df.sentiment, test_size = 0.20, train_size=0.8, random_state = 42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.25, train_size = 0.75, random_state = 42)\n",
    "\n",
    "TEXT_FIELD = data.Field(batch_first = True,\n",
    "                        use_vocab = False,\n",
    "                        sequential=True,\n",
    "                        tokenize=tokenizer, \n",
    "#                   tokenize = tokenize_and_cut,\n",
    "                        preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                        init_token = init_token_idx,\n",
    "                        eos_token = eos_token_idx,\n",
    "                        pad_token = pad_token_idx,\n",
    "                        unk_token = unk_token_idx)\n",
    "\n",
    "LABEL_FIELD = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e4ea98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29675\n",
      "Number of validation examples: 9892\n",
      "Number of testing examples: 9892\n",
      "{'text': [100, 100, 100], 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = DataFrameDataset.splits(\n",
    "    text_field=TEXT_FIELD, label_field=LABEL_FIELD, \n",
    "    train_df = pd.DataFrame({\"text\": X_train, \"sentiment\": Y_train}),\n",
    "    val_df = pd.DataFrame({\"text\": X_val, \"sentiment\": Y_val}), \n",
    "    test_df = pd.DataFrame({\"text\": X_test, \"sentiment\": Y_test}))\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(val_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")\n",
    "\n",
    "print(vars(train_data.examples[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d3e048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]']\n",
      "defaultdict(None, {1: 0, 0: 1})\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "LABEL_FIELD.build_vocab(train_data)\n",
    "\n",
    "print(LABEL_FIELD.vocab.stoi)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ce5d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 2\n",
    "\n",
    "model = BERTGRUSentimentB(bert, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8e64678",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentimentS(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15082b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 111,682,305 trainable parameters\n",
      "The model has 2,759,169 trainable parameters\n",
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n",
      "rnn.bias_ih_l0\n",
      "rnn.bias_hh_l0\n",
      "rnn.weight_ih_l0_reverse\n",
      "rnn.weight_hh_l0_reverse\n",
      "rnn.bias_ih_l0_reverse\n",
      "rnn.bias_hh_l0_reverse\n",
      "rnn.weight_ih_l1\n",
      "rnn.weight_hh_l1\n",
      "rnn.bias_ih_l1\n",
      "rnn.bias_hh_l1\n",
      "rnn.weight_ih_l1_reverse\n",
      "rnn.weight_hh_l1_reverse\n",
      "rnn.bias_ih_l1_reverse\n",
      "rnn.bias_hh_l1_reverse\n",
      "out.weight\n",
      "out.bias\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False\n",
    "        \n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        \n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18110e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 4m 23s\n",
      "\tTrain Loss: 0.700 | Train Acc: 49.92%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 50.00%\n",
      "Epoch: 02 | Epoch Time: 4m 27s\n",
      "\tTrain Loss: 0.695 | Train Acc: 49.75%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.00%\n",
      "Epoch: 03 | Epoch Time: 4m 27s\n",
      "\tTrain Loss: 0.694 | Train Acc: 49.50%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.00%\n",
      "Epoch: 04 | Epoch Time: 4m 27s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.30%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.00%\n",
      "Epoch: 05 | Epoch Time: 4m 27s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.99%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.00%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), './model/model_pt-bert.pt')\n",
    "        torch.save(model, './model/model_pt-bert.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a60aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=True)\n",
    "\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "parameters = {}\n",
    "parameters['init_token_idx'] = init_token_idx\n",
    "parameters['eos_token_idx'] = eos_token_idx\n",
    "parameters['pad_token_idx'] = pad_token_idx\n",
    "parameters['unk_token_idx'] = unk_token_idx\n",
    "parameters['device'] = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "731545d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> ruim\n",
      "negativo =>  0.50%\n",
      "input> bom\n",
      "negativo =>  0.50%\n",
      "input> ótimo\n",
      "negativo =>  0.49%\n",
      "input> maravilhoso\n",
      "negativo =>  0.50%\n",
      "input> exit\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('./model/model_pt-bert.pt'))\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# model = model.to(device)\n",
    "# criterion = criterion.to(device)\n",
    "\n",
    "# test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "# print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "max_input_length = 300\n",
    "\n",
    "while True:\n",
    "    sentence = input(\"input> \")\n",
    "\n",
    "    if sentence == \"exit\":\n",
    "        break\n",
    "\n",
    "    sentiment = predict_sentiment(model, tokenizer, sentence, parameters)\n",
    "\n",
    "    if(np.argmax(sentiment) == 0):\n",
    "        pred_proba = \"%.2f%%\" % (sentiment)\n",
    "        print(\"negativo => \", pred_proba)\n",
    "    elif (np.argmax(sentiment) == 1):\n",
    "        pred_proba = \"%.2f%%\" % (sentiment)\n",
    "        print(\"positivo => \", pred_proba)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7088e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> ruim\n",
      "negativo =>  0.50%\n",
      "input> exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from lib.utils import BERTGRUSentimentB\n",
    "from lib.utils import predict_sentiment\n",
    "\n",
    "model = torch.load('./model/model_pt-bert.pt')\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "max_input_length = 300\n",
    "\n",
    "while True:\n",
    "    sentence = input(\"input> \")\n",
    "\n",
    "    if sentence == \"exit\":\n",
    "        break\n",
    "        \n",
    "    sentiment = predict_sentiment(model, tokenizer, sentence, parameters)\n",
    "\n",
    "    if(np.argmax(sentiment) == 0):\n",
    "        pred_proba = \"%.2f%%\" % (sentiment)\n",
    "        print(\"negativo => \", pred_proba)\n",
    "    elif (np.argmax(sentiment) == 1):\n",
    "        pred_proba = \"%.2f%%\" % (sentiment)\n",
    "        print(\"positivo => \", pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b97d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
