{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e651b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from torchtext.legacy import datasets\n",
    "from torchtext.legacy import data\n",
    "\n",
    "from lib.utils import clean_str\n",
    "from lib.utils import load_data\n",
    "from lib.utils import prepare_data\n",
    "from lib.utils import model \n",
    "\n",
    "from lib.utils import train\n",
    "from lib.utils import evaluate\n",
    "from lib.utils import epoch_time\n",
    "from lib.utils import binary_accuracy\n",
    "from lib.utils import count_parameters\n",
    "from lib.utils import predict_sentiment\n",
    "from lib.utils import BERTGRUSentimentS\n",
    "from lib.utils import BERTGRUSentimentB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 7\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Fonte da classe DataFrameDataset: https://gist.github.com/nissan/ccb0553edb6abafd20c3dec34ee8099d\n",
    "\n",
    "class DataFrameDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row.sentiment if not is_test else None\n",
    "            text = row.text\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, train_df, val_df=None, test_df=None, **kwargs):\n",
    "        train_data, val_data, test_data = (None, None, None)\n",
    "\n",
    "        if train_df is not None:\n",
    "            train_data = cls(train_df.copy(), text_field, label_field, **kwargs)\n",
    "        if val_df is not None:\n",
    "            val_data = cls(val_df.copy(), text_field, label_field, **kwargs)\n",
    "        if test_df is not None:\n",
    "            test_data = cls(test_df.copy(), text_field, label_field, True, **kwargs)\n",
    "\n",
    "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27519d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29794\n",
      "<bound method PreTrainedTokenizerFast.convert_tokens_to_ids of PreTrainedTokenizerFast(name_or_path='neuralmind/bert-base-portuguese-cased', vocab_size=29794, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})>\n",
      "[CLS] [SEP] [PAD] [UNK]\n",
      "101 102 0 100\n",
      "Carregando dataset.\n",
      "Dataset carregado.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=True)\n",
    "\n",
    "print(len(tokenizer.vocab))\n",
    "print(tokenizer.convert_tokens_to_ids)\n",
    "\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)\n",
    "\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n",
    "\n",
    "max_input_length = 300\n",
    "\n",
    "parameters = {}\n",
    "parameters['model_filename'] = 'model/model_pt-bi-lstm.h5' # O model será exportado para este arquivo\n",
    "\n",
    "parameters['pre_trained_wv'] = False\n",
    "parameters['bilstm'] = True # LSTM Bidirectional True or False\n",
    "\n",
    "# Repositório local\n",
    "# parameters['dataset_file'] = './dataset/data_imdb_en_pt.csv'\n",
    "\n",
    "# Repositório em nuvem:\n",
    "parameters['dataset_file'] = 'https://1drv.ms/u/s!AtQLEBYHemNkgdt334si6Qepxomgow?e=2w7eEP'\n",
    "\n",
    "parameters['lang'] = 'pt' # pt or en\n",
    "parameters['load_from'] = 'ftr' # csv or ftr\n",
    "\n",
    "parameters['epochs'] = 5\n",
    "\n",
    "parameters['word_embedding_dim'] = 50 # dimensionalidade do word embedding pré-treinado\n",
    "parameters['batch_size'] = 32 # número de amostras a serem utilizadas em cada atualização do gradiente\n",
    "parameters['max_features'] = 5000 # Reflete a quantidade máxima de palavras que iremos manter no vocabulário\n",
    "parameters['embed_dim'] = 128 # dimensão de saída da camada Embedding\n",
    "parameters['max_sequence_length'] = 300 # limitamos o tamanho máximo de todas as sentenças\n",
    "\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens\n",
    "\n",
    "df = load_data(parameters)\n",
    "df.sentiment.replace({\"neg\": 0, \"pos\": 1}, inplace = True)\n",
    "\n",
    "X, X_test, Y, Y_test = train_test_split(df.text,df.sentiment, test_size = 0.20, train_size=0.8, random_state = 42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.25, train_size = 0.75, random_state = 42)\n",
    "\n",
    "TEXT_FIELD = data.Field(batch_first = True,\n",
    "                        use_vocab = False,\n",
    "                        sequential=True,\n",
    "                        tokenize=tokenizer, \n",
    "#                   tokenize = tokenize_and_cut,\n",
    "                        preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                        init_token = init_token_idx,\n",
    "                        eos_token = eos_token_idx,\n",
    "                        pad_token = pad_token_idx,\n",
    "                        unk_token = unk_token_idx)\n",
    "\n",
    "LABEL_FIELD = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1e4ea98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29675\n",
      "Number of validation examples: 9892\n",
      "Number of testing examples: 9892\n",
      "{'text': [100, 100, 100], 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = DataFrameDataset.splits(\n",
    "    text_field=TEXT_FIELD, label_field=LABEL_FIELD, \n",
    "    train_df = pd.DataFrame({\"text\": X_train, \"sentiment\": Y_train}),\n",
    "    val_df = pd.DataFrame({\"text\": X_val, \"sentiment\": Y_val}), \n",
    "    test_df = pd.DataFrame({\"text\": X_test, \"sentiment\": Y_test}))\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(val_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")\n",
    "\n",
    "print(vars(train_data.examples[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68d3e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
    "\n",
    "LABEL_FIELD.build_vocab(train_data)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ce5d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = BERTGRUSentimentB(bert, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e64678",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentimentS(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15082b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,417,986 trainable parameters\n",
      "The model has 5,417,986 trainable parameters\n",
      "gru11.weight_ih_l0\n",
      "gru11.weight_hh_l0\n",
      "gru11.bias_ih_l0\n",
      "gru11.bias_hh_l0\n",
      "gru12.weight_ih_l0\n",
      "gru12.weight_hh_l0\n",
      "gru12.bias_ih_l0\n",
      "gru12.bias_hh_l0\n",
      "gru13.weight_ih_l0\n",
      "gru13.weight_hh_l0\n",
      "gru13.bias_ih_l0\n",
      "gru13.bias_hh_l0\n",
      "gru21.weight_ih_l0\n",
      "gru21.weight_hh_l0\n",
      "gru21.bias_ih_l0\n",
      "gru21.bias_hh_l0\n",
      "gru22.weight_ih_l0\n",
      "gru22.weight_hh_l0\n",
      "gru22.bias_ih_l0\n",
      "gru22.bias_hh_l0\n",
      "gru23.weight_ih_l0\n",
      "gru23.weight_hh_l0\n",
      "gru23.bias_ih_l0\n",
      "gru23.bias_hh_l0\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False\n",
    "        \n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        \n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18110e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 4m 44s\n",
      "\tTrain Loss: 0.694 | Train Acc: 49.94%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.00%\n",
      "Epoch: 02 | Epoch Time: 4m 49s\n",
      "\tTrain Loss: 0.694 | Train Acc: 49.71%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.00%\n",
      "Epoch: 03 | Epoch Time: 4m 49s\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.55%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.00%\n",
      "Epoch: 04 | Epoch Time: 4m 46s\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.81%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.00%\n",
      "Epoch: 05 | Epoch Time: 4m 48s\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.44%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.00%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), './model/model_pt-bert.pt')\n",
    "        torch.save(model, './model/model_pt-bert.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3a60aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=True)\n",
    "\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "parameters = {}\n",
    "parameters['init_token_idx'] = init_token_idx\n",
    "parameters['eos_token_idx'] = eos_token_idx\n",
    "parameters['pad_token_idx'] = pad_token_idx\n",
    "parameters['unk_token_idx'] = unk_token_idx\n",
    "parameters['device'] = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "731545d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> postura\n",
      "negativo =>  0.49%\n",
      "input> olha só\n",
      "negativo =>  0.49%\n",
      "input> bom demais\n",
      "negativo =>  0.49%\n",
      "input> eita\n",
      "negativo =>  0.49%\n",
      "input> vixi\n",
      "negativo =>  0.49%\n",
      "input> exit\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('./model/model_pt-bert.pt'))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "# print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "max_input_length = 300\n",
    "\n",
    "while True:\n",
    "    sentence = input(\"input> \")\n",
    "\n",
    "    if sentence == \"exit\":\n",
    "        break\n",
    "\n",
    "    sentiment = predict_sentiment(model, tokenizer, sentence, parameters)\n",
    "\n",
    "    if(np.argmax(sentiment) == 0):\n",
    "        pred_proba = \"%.2f%%\" % (sentiment)\n",
    "        print(\"negativo => \", pred_proba)\n",
    "    elif (np.argmax(sentiment) == 1):\n",
    "        pred_proba = \"%.2f%%\" % (sentiment)\n",
    "        print(\"positivo => \", pred_proba)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7088e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> oi\n",
      "negativo =>  0.49%\n",
      "input> exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from lib.utils import BERTGRUSentimentB\n",
    "from lib.utils import predict_sentiment\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=True)\n",
    "\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "parameters = {}\n",
    "parameters['init_token_idx'] = init_token_idx\n",
    "parameters['eos_token_idx'] = eos_token_idx\n",
    "parameters['pad_token_idx'] = pad_token_idx\n",
    "parameters['unk_token_idx'] = unk_token_idx\n",
    "parameters['device'] = device\n",
    "\n",
    "\n",
    "model = torch.load('./model/model_pt-bert.pt')\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "max_input_length = 300\n",
    "\n",
    "while True:\n",
    "    sentence = input(\"input> \")\n",
    "\n",
    "    if sentence == \"exit\":\n",
    "        break\n",
    "        \n",
    "    sentiment = predict_sentiment(model, tokenizer, sentence, parameters)\n",
    "\n",
    "    if(np.argmax(sentiment) == 0):\n",
    "        pred_proba = \"%.2f%%\" % (sentiment)\n",
    "        print(\"negativo => \", pred_proba)\n",
    "    elif (np.argmax(sentiment) == 1):\n",
    "        pred_proba = \"%.2f%%\" % (sentiment)\n",
    "        print(\"positivo => \", pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3cca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
