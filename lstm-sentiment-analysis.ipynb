{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0985b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# !pip install transformers\n",
    "# !pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f177e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset.\n",
      "Dataset carregado.\n",
      "\n",
      "Preparando dados de treinamento.\n",
      "Dados separados!\n",
      "\n",
      "(39567, 300) (39567, 2)\n",
      "(9892, 300) (9892, 2)\n",
      "\n",
      "Construindo modelo...\n",
      "Feito!\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 300)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 300, 128)          640000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 903,682\n",
      "Trainable params: 903,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Preparando utilização do modelo.\n",
      "\n",
      "Acc: 87.53%\n",
      "input> ótimo\n",
      "1/1 - 0s\n",
      "negativo =>  57.12%\n",
      "input> ótimo é muito bom\n",
      "1/1 - 0s\n",
      "positivo =>  92.74%\n",
      "input> poucas palavras não gera uma avaliação boa.\n",
      "1/1 - 0s\n",
      "positivo =>  90.66%\n",
      "input> poucas palavras\n",
      "1/1 - 0s\n",
      "negativo =>  72.21%\n",
      "input> bad\n",
      "1/1 - 0s\n",
      "negativo =>  99.74%\n",
      "input> exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.utils import clean_str\n",
    "from lib.utils import load_data\n",
    "from lib.utils import prepare_data\n",
    "from lib.utils import model \n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "parameters = {}\n",
    "parameters['model_filename'] = 'model/model_pt-bi-lstm.h5' # O model será exportado para este arquivo\n",
    "parameters['pre_trained_wv'] = False\n",
    "parameters['bilstm'] = True # LSTM Bidirectional True or False\n",
    "\n",
    "# parameters['dataset_file'] = './dataset/data_imdb_en_pt.csv'\n",
    "parameters['dataset_file'] = 'https://1drv.ms/u/s!AtQLEBYHemNkgdt334si6Qepxomgow?e=2w7eEP'\n",
    "\n",
    "parameters['lang'] = 'pt' # pt or en\n",
    "parameters['load_from'] = 'ftr' # csv or ftr\n",
    "\n",
    "parameters['epochs'] = 5\n",
    "\n",
    "parameters['word_embedding_dim'] = 50 # dimensionalidade do word embedding pré-treinado\n",
    "parameters['batch_size'] = 32 # número de amostras a serem utilizadas em cada atualização do gradiente\n",
    "parameters['max_features'] = 5000 # Reflete a quantidade máxima de palavras que iremos manter no vocabulário\n",
    "parameters['embed_dim'] = 128 # dimensão de saída da camada Embedding\n",
    "parameters['max_sequence_length'] = 300 # limitamos o tamanho máximo de todas as sentenças\n",
    "\n",
    "\n",
    "data = load_data(parameters)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, word_index, tokenizer = prepare_data(data, parameters)\n",
    "\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n",
    "\n",
    "model = model(parameters)\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "if not os.path.exists('./{}'.format(parameters['model_filename']) ):\n",
    "\n",
    "    hist = model.fit(\n",
    "        X_train, \n",
    "        Y_train, \n",
    "        validation_data=(X_test, Y_test),\n",
    "        epochs=parameters['epochs'],\n",
    "        batch_size=parameters['batch_size'], \n",
    "        shuffle=True,\n",
    "        verbose=1)\n",
    "\n",
    "    model.save_weights(parameters['model_filename'])    \n",
    "\n",
    "\n",
    "    # Plot\n",
    "    plt.figure()\n",
    "    plt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\n",
    "    plt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\n",
    "    plt.title('Classificador de sentimentos')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Cross-Entropy')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(hist.history['accuracy'], lw=2.0, color='b', label='train')\n",
    "    plt.plot(hist.history['val_accuracy'], lw=2.0, color='r', label='val')\n",
    "    plt.title('Classificador de sentimentos')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    model.load_weights('./{}'.format(parameters['model_filename']) )\n",
    "\n",
    "print(\"Preparando utilização do modelo.\\n\")\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, verbose = 0, batch_size = parameters['batch_size'])\n",
    "print(\"Acc: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "while True:\n",
    "    sentence = input(\"input> \")\n",
    "\n",
    "    if sentence == \"exit\":\n",
    "        break\n",
    "    \n",
    "    new_text = [sentence]\n",
    "    new_text = tokenizer.texts_to_sequences(new_text)\n",
    "\n",
    "    new_text = pad_sequences(new_text, maxlen=parameters['max_sequence_length'], dtype='int32', value=0)\n",
    "\n",
    "    sentiment = model.predict(new_text,batch_size=1,verbose = 2)[0]\n",
    "\n",
    "    if(np.argmax(sentiment) == 0):\n",
    "        pred_proba = \"%.2f%%\" % (sentiment[0]*100)\n",
    "        print(\"negativo => \", pred_proba)\n",
    "    elif (np.argmax(sentiment) == 1):\n",
    "        pred_proba = \"%.2f%%\" % (sentiment[1]*100)\n",
    "        print(\"positivo => \", pred_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
